{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:48:39.020217Z",
     "start_time": "2025-05-15T07:48:39.002274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ],
   "id": "faed6b45c55e7636",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:49:28.736761Z",
     "start_time": "2025-05-15T07:49:28.723309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    dataset = load_dataset(\"daily_dialog\")\n",
    "    all_sentences = []\n",
    "    all_emotions = []\n",
    "\n",
    "    for example in dataset[\"train\"]:\n",
    "        dialogues = example[\"dialog\"]\n",
    "        emotions = example[\"emotion\"]\n",
    "\n",
    "        for sentence, emotion in zip(dialogues, emotions):\n",
    "            all_sentences.append(sentence)\n",
    "            all_emotions.append(emotion)\n",
    "\n",
    "    return all_sentences, all_emotions"
   ],
   "id": "973bfb7257492e3d",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:49:29.305962Z",
     "start_time": "2025-05-15T07:49:29.283122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data(sentences, emotion_labels, train_ratio=0.8, seed=5):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_samples = len(sentences)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    split_point = int(train_ratio * n_samples)\n",
    "\n",
    "    train_indices = indices[:split_point]\n",
    "    test_indices = indices[split_point:]\n",
    "\n",
    "    train_sentences = [sentences[i] for i in train_indices]\n",
    "    train_emotions = [emotion_labels[i] for i in train_indices]\n",
    "    test_sentences = [sentences[i] for i in test_indices]\n",
    "    test_emotions = [emotion_labels[i] for i in test_indices]\n",
    "\n",
    "    print(f\"Number of training sentences: {len(train_sentences)}\")\n",
    "    print(f\"Number of test sentences: {len(test_sentences)}\")\n",
    "    print()\n",
    "    return train_sentences, train_emotions, test_sentences, test_emotions\n"
   ],
   "id": "e5d2cdca61be78c8",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:49:29.940671Z",
     "start_time": "2025-05-15T07:49:29.926036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_bow_features(trainInputs, testInputs):\n",
    "    print(f\"Number of train sentences: {len(train_sentences)}\")\n",
    "    print(f\"Number of test sentences: {len(test_sentences)}\")\n",
    "\n",
    "    train_features = vectorizer.fit_transform(train_sentences)\n",
    "\n",
    "    test_features = vectorizer.transform(test_sentences)\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)} words\")\n",
    "    print(f\"Training features shape: {train_features.shape}\")\n",
    "    print(f\"Testing features shape: {test_features.shape}\")\n",
    "    print()\n",
    "    return train_features, test_features"
   ],
   "id": "f16eb53f70d7a5aa",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:49:30.396044Z",
     "start_time": "2025-05-15T07:49:30.391261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(train_features, train_emotions):\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "    classifier.fit(train_features, train_emotions)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def test_model(classifier, test_features, test_emotions):\n",
    "    predictions = classifier.predict(test_features)\n",
    "    accuracy = accuracy_score(test_emotions, predictions)\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n"
   ],
   "id": "b2091f2b68a9a1a1",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:49:42.641486Z",
     "start_time": "2025-05-15T07:49:30.939322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    sentences, labels = load_data()\n",
    "    train_sentences, train_emotions, test_sentences, test_emotions = split_data(sentences, labels)\n",
    "    print()\n",
    "    train_features, test_features = extract_bow_features(train_sentences, test_sentences)\n",
    "\n",
    "    clf = train_model(train_features, train_emotions)\n",
    "    test_model(clf, test_features, test_emotions)\n",
    "\n"
   ],
   "id": "f94101faa84e25f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87170 sentences from DailyDialog dataset\n",
      "\n",
      "Number of training sentences: 69736\n",
      "Number of test sentences: 17434\n",
      "\n",
      "\n",
      "Number of train sentences: 69736\n",
      "Number of test sentences: 17434\n",
      "Vocabulary size: 16428 words\n",
      "Training features shape: (69736, 16428)\n",
      "Testing features shape: (17434, 16428)\n",
      "\n",
      "Test accuracy: 0.8460\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d60e2a92cc262d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
